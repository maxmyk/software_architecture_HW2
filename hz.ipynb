{"cells":[{"cell_type":"markdown","metadata":{"id":"uYSAaNS-61j6"},"source":["# Hazelcast HW\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Встановити і налаштувати Hazelcast"]},{"cell_type":"markdown","metadata":{"id":"M7Tv7PIyzYBF"},"source":["## 2. Сконфігурувати і запустити 3 ноди (інстанси) об'єднані в кластер або як частину Java-застосування, або як окремі застосування"]},{"cell_type":"markdown","metadata":{},"source":["I've done it using three Docker instances.\n","```\n","docker run -it --name first-hazelcast-node  --rm -e HZ_NETWORK_PUBLICADDRESS=172.17.0.1:5701 -e HZ_CLUSTERNAME=hazelcast-cluster -p 5701:5701 hazelcast/hazelcast:5.3.6\n","docker run -it --name second-hazelcast-node --rm -e HZ_NETWORK_PUBLICADDRESS=172.17.0.1:5702 -e HZ_CLUSTERNAME=hazelcast-cluster -p 5702:5701 hazelcast/hazelcast:5.3.6\n","docker run -it --name third-hazelcast-node  --rm -e HZ_NETWORK_PUBLICADDRESS=172.17.0.1:5703 -e HZ_CLUSTERNAME=hazelcast-cluster -p 5703:5701 hazelcast/hazelcast:5.3.6\n","```\n","\n","you can run them simultaneously using ```run.sh```"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Продемонструйте роботу Distributed Map"]},{"cell_type":"markdown","metadata":{},"source":["### Використовуючи API створіть Distributed Map + запишіть в неї 1000 значень з ключем від 0 до 1к"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Lifecycle event >>> STARTING\n","Lifecycle event >>> STARTED\n","Lifecycle event >>> CONNECTED\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000/1000 [00:00<00:00, 1899.77it/s]"]},{"name":"stdout","output_type":"stream","text":["Lifecycle event >>> SHUTTING_DOWN\n","Lifecycle event >>> DISCONNECTED\n","Lifecycle event >>> SHUTDOWN\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import hazelcast\n","from tqdm import tqdm\n","\n","client = hazelcast.HazelcastClient(\n","    cluster_name=\"hazelcast-cluster\",\n","    cluster_members=[\n","        \"172.17.0.1:5701\",\n","        \"172.17.0.1:5702\",\n","        \"172.17.0.1:5703\",\n","    ],\n","    lifecycle_listeners=[\n","        lambda state: print(\"Lifecycle event >>>\", state),\n","    ]\n",")\n","# Step 1: Create a Distributed Map\n","distributed_map = client.get_map(\"my-distributed-map\").blocking()\n","\n","# Step 2: Write 1000 values to the Distributed Map\n","for i in tqdm(range(1000)):\n","    distributed_map.put(i, f\"value-{i}\")\n","\n","client.shutdown()"]},{"cell_type":"markdown","metadata":{},"source":["### За допомогою Management Center подивиться на розподіл значень по нодах\n","[image  ]"]},{"cell_type":"markdown","metadata":{},"source":["### Як зміниться розподіл даних по нодах\n","\n","- якщо відключити одну ноду\n","\n","Дані перерозподіляться.\n","[image]\n","\n","- відключити дві ноди.\n","\n","Якщо зробити це по черзі, то дані збережуться, якщо одночасно, то частина даних втратиться.\n","[image]\n","\n","- Чи буде втрата даних?\n","\n","Якщо багато вузлів відключаться одночасно, не буде з чого відновити дані.\n","\n","- Яким чином зробити щоб не було втрати даних?\n","\n","Щоб уникнути втрати даних, треба, щоб кластер мав достатню кількість резервних вузлів.\n","Це гарантує, що навіть якщо один або декілька вузлів вийде з ладу, дані все одно можна буде відновити з резервних."]},{"cell_type":"markdown","metadata":{},"source":["## 4. Продемонструйте роботу Distributed Map with locks"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# helpers for testing the locking\n","\n","import multiprocessing\n","from hazelcast import HazelcastClient\n","from tqdm import tqdm\n","\n","verbose = False\n","\n","# =============================================================================\n","\n","def increment_value():\n","    client = HazelcastClient(\n","        cluster_name=\"hazelcast-cluster\",\n","        cluster_members=[\n","            \"172.17.0.1:5701\",\n","            \"172.17.0.1:5702\",\n","            \"172.17.0.1:5703\",\n","        ],\n","        lifecycle_listeners=[\n","            lambda state: print(\"Lifecycle event >>>\", state) if verbose else None,\n","        ]\n","    )\n","    map = client.get_map(\"my-distributed-map\").blocking()\n","\n","    for _ in tqdm(range(10000), disable=(not verbose)):\n","        value = map.get(\"key\")\n","        value += 1\n","        map.put(\"key\", value)\n","    if verbose:\n","        print(map.get(\"key\"))\n","    return map.get(\"key\").__str__()\n","\n","def increment_value_pl(): # pl = pesimistic lock\n","    client = HazelcastClient(\n","        cluster_name=\"hazelcast-cluster\",\n","        cluster_members=[\n","            \"172.17.0.1:5701\",\n","            \"172.17.0.1:5702\",\n","            \"172.17.0.1:5703\",\n","        ],\n","        lifecycle_listeners=[\n","            lambda state: print(\"Lifecycle event >>>\", state) if verbose else None,\n","        ]\n","    )\n","    map = client.get_map(\"my-distributed-map\").blocking()\n","    last_value = 0\n","    for _ in tqdm(range(10000), disable=(not verbose)):\n","        map.lock(\"key\")\n","        try:\n","            value = map.get(\"key\")\n","            value += 1\n","            map.put(\"key\", value)\n","            last_value = value\n","        finally:\n","            map.unlock(\"key\")\n","    if verbose:\n","        print(last_value)\n","    return last_value\n","    \n","def increment_value_ol(): # ol = optimistic lock\n","    client = HazelcastClient(\n","        cluster_name=\"hazelcast-cluster\",\n","        cluster_members=[\n","            \"172.17.0.1:5701\",\n","            \"172.17.0.1:5702\",\n","            \"172.17.0.1:5703\",\n","        ],\n","        lifecycle_listeners=[\n","            lambda state: print(\"Lifecycle event >>>\", state) if verbose else None,\n","        ]\n","    )\n","    map = client.get_map(\"my-distributed-map\").blocking()\n","    last_value = 0\n","    \n","    for _ in tqdm(range(10000), disable=(not verbose)):\n","        while True:\n","            value = map.get(\"key\")\n","            value += 1\n","            last_value = value\n","            if map.replace_if_same(\"key\", value - 1, value):\n","                break\n","    if verbose:\n","        print(last_value)\n","    return last_value\n","\n","# =============================================================================\n","\n","def test_func(func, num_processes=3):\n","    processes = []\n","    for _ in range(num_processes):\n","        p = multiprocessing.Process(target=func)\n","        p.start()\n","        processes.append(p)\n","    for p in processes:\n","        p.join()\n","\n","def test_increment_value():\n","    test_func(increment_value)\n","\n","def test_increment_value_pl():\n","    test_func(increment_value_pl)\n","    \n","def test_increment_value_ol():\n","    test_func(increment_value_ol)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Lifecycle event >>> STARTING\n","Lifecycle event >>> STARTED\n","Lifecycle event >>> CONNECTED\n","Test 1, unlocked increment finished. Final key value: 11963\n","Test 2, pessimistic locking increment finished. Final key value: 30000\n","Test 3, optimistic locking increment finished. Final key value: 30000\n","Lifecycle event >>> SHUTTING_DOWN\n","Lifecycle event >>> DISCONNECTED\n","Lifecycle event >>> SHUTDOWN\n"]}],"source":["client = hazelcast.HazelcastClient(\n","    cluster_name=\"hazelcast-cluster\",\n","    cluster_members=[\n","        \"172.17.0.1:5701\",\n","        \"172.17.0.1:5702\",\n","        \"172.17.0.1:5703\",\n","    ],\n","    lifecycle_listeners=[\n","        lambda state: print(\"Lifecycle event >>>\", state),\n","    ]\n",")\n","def test_prep():\n","    client.get_map(\"my-distributed-map\").blocking().clear()\n","    my_map = client.get_map(\"my-distributed-map\")\n","    my_map.put_if_absent(\"key\", 0)\n","    return my_map\n","\n","my_map = test_prep()\n","test_increment_value()\n","print(\"Test 1, unlocked increment finished. Final key value:\", my_map.get(\"key\").result())\n","\n","my_map = test_prep()\n","test_increment_value_pl()\n","print(\"Test 2, pessimistic locking increment finished. Final key value:\", my_map.get(\"key\").result())\n","\n","my_map = test_prep()\n","test_increment_value_ol()\n","print(\"Test 3, optimistic locking increment finished. Final key value:\", my_map.get(\"key\").result())\n","\n","client.shutdown()"]},{"cell_type":"markdown","metadata":{},"source":["### Порівняйте результати кожного з запусків\n","В реалізації без блокувань спостерігається втрата даних, а у реалізаціях з песимістичним та оптимістичним блокуванням є однакові результати, як і очікувано.\n","\n","```\n","Test 1, unlocked increment finished. Final key value: ~13000\n","Test 2, pessimistic locking increment finished. Final key value: 30000\n","Test 3, optimistic locking increment finished. Final key value: 30000\n","```"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Робота з Bounded queue"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["P Lifecycle event >>> STARTING\n","P Lifecycle event >>> STARTED\n","P Lifecycle event >>> CONNECTED\n","P Lifecycle event >>> SHUTTING_DOWN\n","P Lifecycle event >>> DISCONNECTED\n","P Lifecycle event >>> SHUTDOWN\n"]}],"source":["# # import threading, hazelcast\n","# # client = hazelcast.HazelcastClient(\n","# #     cluster_name=\"hazelcast-cluster\",\n","# #     cluster_members=[\n","# #         \"172.17.0.1:5701\",\n","# #         \"172.17.0.1:5702\",\n","# #         \"172.17.0.1:5703\",\n","# #     ],\n","# #     lifecycle_listeners=[\n","# #         lambda state: print(\"Lifecycle event >>>\", state),\n","# #     ]\n","# # )\n","\n","# # queue = client.get_queue(\"queue\") \n","\n","# # def produce():\n","# #     for i in range(100):\n","# #         queue.offer(i)\n","\n","\n","# # producer_thread = threading.Thread(target=produce)\n","\n","# # producer_thread.start()\n","\n","# # producer_thread.join()\n","\n","\n","# # def consume():\n","# #     consumed_count = 0\n","# #     while consumed_count < 50: \n","# #         head = queue.take().result()\n","# #         print(\"Consuming {}\".format(head))\n","# #         consumed_count += 1\n","\n","\n","# # consumer_thread = threading.Thread(target=consume)\n","\n","# # consumer_thread.start()\n","\n","# # second_consumer_thread = threading.Thread(target=consume)\n","\n","# # second_consumer_thread.start()\n","\n","\n","# # consumer_thread.join()\n","# # second_consumer_thread.join()\n","\n","# # client.shutdown()\n","\n","# import hazelcast\n","# import concurrent.futures\n","\n","# def producer():\n","#     # Connect to Hazelcast cluster\n","#     client = hazelcast.HazelcastClient(cluster_name=\"hazelcast-cluster\",\n","#     cluster_members=[\n","#         \"172.17.0.1:5701\",\n","#         \"172.17.0.1:5702\",\n","#         \"172.17.0.1:5703\",\n","#     ],\n","#     lifecycle_listeners=[\n","#         lambda state: print(\"Prod Lifecycle event >>>\", state),\n","#     ])\n","\n","#     # Create or get the bounded queue\n","#     queue = client.get_queue(\"bounded-queue\")\n","\n","#     # Write values 1 to 100 to the queue\n","#     for i in range(1, 101):\n","#         queue.put(i)\n","\n","#     # Close the client connection\n","#     client.shutdown()\n","\n","# def consumer(client_id):\n","#     # Connect to Hazelcast cluster\n","#     client = hazelcast.HazelcastClient(cluster_name=\"hazelcast-cluster\",\n","#     cluster_members=[\n","#         \"172.17.0.1:5701\",\n","#         \"172.17.0.1:5702\",\n","#         \"172.17.0.1:5703\",\n","#     ],\n","#     lifecycle_listeners=[\n","#         lambda state: print(\"Cons Lifecycle event >>>\", state),\n","#     ])\n","\n","#     # Create or get the bounded queue\n","#     queue = client.get_queue(\"bounded-queue\")\n","\n","#     # Read values from the queue\n","#     for _ in range(150):\n","#         try:\n","#             value = queue.take()\n","#             print(f\"Client {client_id} - Received: {value.result()}\")\n","#         except hazelcast.errors.HazelcastQueueError:\n","#             print(f\"Client {client_id} - Queue is empty. Waiting for items...\")\n","\n","# # Run producer and consumers simultaneously\n","# with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n","#     # Start producer\n","#     producer_future = executor.submit(producer)\n","\n","#     # Start two consumers\n","#     consumer_futures = [executor.submit(consumer, i) for i in range(1, 3)]\n","\n","#     # Wait for all tasks to complete\n","#     concurrent.futures.wait([producer_future] + consumer_futures)\n","import hazelcast\n","\n","def main():\n","    client = hazelcast.HazelcastClient(cluster_name=\"hazelcast-cluster\",\n","    cluster_members=[\n","        \"172.17.0.1:5701\"\n","    ],\n","    lifecycle_listeners=[\n","        lambda state: print(\"P Lifecycle event >>>\", state),\n","    ])\n","    queue = client.get_queue(\"boundedQueue\")\n","\n","    for i in range(1, 101):\n","        queue.put(i)\n","\n","    client.shutdown()\n","main()\n","# if __name__ == \"__main__\":\n","    # main()\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1fVjQE2dntoN0adCYGZsassDllLur2Phi","timestamp":1679311917076}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
